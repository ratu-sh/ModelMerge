import os
import re
import datetime
import requests
import threading
import time as record_time
from itertools import islice
from bs4 import BeautifulSoup
from duckduckgo_search import DDGS
from googleapiclient.discovery import build

class ThreadWithReturnValue(threading.Thread):
    def run(self):
        if self._target is not None:
            self._return = self._target(*self._args, **self._kwargs)

    def join(self):
        super().join()
        return self._return

def Web_crawler(url: str, isSearch=False) -> str:
    """è¿”å›é“¾æ¥ç½‘å€urlæ­£æ–‡å†…å®¹ï¼Œå¿…é¡»æ˜¯åˆæ³•çš„ç½‘å€"""
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3"
    }
    result = ''
    try:
        requests.packages.urllib3.disable_warnings()
        response = requests.get(url, headers=headers, verify=False, timeout=3, stream=True)
        if response.status_code == 404:
            print("Page not found:", url)
            return ""
            # return "æŠ±æ­‰ï¼Œç½‘é¡µä¸å­˜åœ¨ï¼Œç›®å‰æ— æ³•è®¿é—®è¯¥ç½‘é¡µã€‚@Trash@"
        content_length = int(response.headers.get('Content-Length', 0))
        if content_length > 5000000:
            print("Skipping large file:", url)
            return result
        soup = BeautifulSoup(response.text.encode(response.encoding), 'lxml', from_encoding='utf-8')

        table_contents = ""
        tables = soup.find_all('table')
        for table in tables:
            table_contents += table.get_text()
            table.decompose()
        body = "".join(soup.find('body').get_text().split('\n'))
        result = table_contents + body
        if result == '' and not isSearch:
            result = ""
            # result = "æŠ±æ­‰ï¼Œå¯èƒ½åçˆ¬è™«ç­–ç•¥ï¼Œç›®å‰æ— æ³•è®¿é—®è¯¥ç½‘é¡µã€‚@Trash@"
        if result.count("\"") > 1000:
            result = ""
    except Exception as e:
        print('\033[31m')
        print("error url", url)
        print("error", e)
        print('\033[0m')
    # print("url content", result + "\n\n")
    return result

def jina_ai_Web_crawler(url: str, isSearch=False) -> str:
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3"
    }
    result = ''
    try:
        requests.packages.urllib3.disable_warnings()
        url = "https://r.jina.ai/" + url
        response = requests.get(url, headers=headers, verify=False, timeout=5, stream=True)
        if response.status_code == 404:
            print("Page not found:", url)
            return "æŠ±æ­‰ï¼Œç½‘é¡µä¸å­˜åœ¨ï¼Œç›®å‰æ— æ³•è®¿é—®è¯¥ç½‘é¡µã€‚@Trash@"
        content_length = int(response.headers.get('Content-Length', 0))
        if content_length > 5000000:
            print("Skipping large file:", url)
            return result
        soup = BeautifulSoup(response.text.encode(response.encoding), 'lxml', from_encoding='utf-8')

        table_contents = ""
        tables = soup.find_all('table')
        for table in tables:
            table_contents += table.get_text()
            table.decompose()
        body = "".join(soup.find('body').get_text().split('\n'))
        result = table_contents + body
        if result == '' and not isSearch:
            result = "æŠ±æ­‰ï¼Œå¯èƒ½åçˆ¬è™«ç­–ç•¥ï¼Œç›®å‰æ— æ³•è®¿é—®è¯¥ç½‘é¡µã€‚@Trash@"
        if result.count("\"") > 1000:
            result = ""
    except Exception as e:
        print('\033[31m')
        print("error url", url)
        print("error", e)
        print('\033[0m')
    # print(result + "\n\n")
    return result

def getddgsearchurl(query, max_results=4):
    try:
        results = []
        with DDGS() as ddgs:
            ddgs_gen = ddgs.text(query, safesearch='Off', timelimit='y', backend="lite")
            for r in islice(ddgs_gen, max_results):
                results.append(r)
        urls = [result['href'] for result in results]
    except Exception as e:
        print('\033[31m')
        print("duckduckgo error", e)
        print('\033[0m')
        urls = []
    return urls

def getgooglesearchurl(result, numresults=3):
    urls = []
    if os.environ.get('GOOGLE_API_KEY', None) == None and os.environ.get('GOOGLE_CSE_ID', None) == None:
        return urls
    try:
        service = build("customsearch", "v1", developerKey=os.environ.get('GOOGLE_API_KEY', None))
        res = service.cse().list(q=result, cx=os.environ.get('GOOGLE_CSE_ID', None)).execute()
        link_list = [item['link'] for item in res['items']]
        urls = link_list[:numresults]
    except Exception as e:
        print('\033[31m')
        print("error", e)
        print('\033[0m')
        if "rateLimitExceeded" in str(e):
            print("Google API æ¯æ—¥è°ƒç”¨é¢‘ç‡å·²è¾¾ä¸Šé™ï¼Œè¯·æ˜æ—¥å†è¯•ï¼")
    # print("google urls", urls)
    return urls

def sort_by_time(urls):
    def extract_date(url):
        match = re.search(r'[12]\d{3}.\d{1,2}.\d{1,2}', url)
        if match is not None:
            match = re.sub(r'([12]\d{3}).(\d{1,2}).(\d{1,2})', "\\1/\\2/\\3", match.group())
            print(match)
            if int(match[:4]) > datetime.datetime.now().year:
                match = "1000/01/01"
        else:
            match = "1000/01/01"
        try:
            return datetime.datetime.strptime(match, '%Y/%m/%d')
        except:
            match = "1000/01/01"
            return datetime.datetime.strptime(match, '%Y/%m/%d')

    # æå–æ—¥æœŸå¹¶åˆ›å»ºä¸€ä¸ªåŒ…å«æ—¥æœŸå’ŒURLçš„å…ƒç»„åˆ—è¡¨
    date_url_pairs = [(extract_date(url), url) for url in urls]

    # æŒ‰æ—¥æœŸæ’åº
    date_url_pairs.sort(key=lambda x: x[0], reverse=True)

    # è·å–æ’åºåçš„URLåˆ—è¡¨
    sorted_urls = [url for _, url in date_url_pairs]

    return sorted_urls

async def get_search_url(keywords, search_url_num):
    yield "ğŸŒ message_search_stage_2"

    search_threads = []
    search_thread = ThreadWithReturnValue(target=getgooglesearchurl, args=(keywords[0],search_url_num,))
    search_thread.start()
    search_threads.append(search_thread)
    keywords.pop(0)

    urls_set = []
    urls_set += getddgsearchurl(keywords[0], search_url_num)

    for t in search_threads:
        tmp = t.join()
        urls_set += tmp
    url_set_list = sorted(set(urls_set), key=lambda x: urls_set.index(x))
    url_set_list = sort_by_time(url_set_list)

    url_pdf_set_list = [item for item in url_set_list if item.endswith(".pdf")]
    url_set_list = [item for item in url_set_list if not item.endswith(".pdf")]
    # cut_num = int(len(url_set_list) * 1 / 3)
    yield url_set_list[:6], url_pdf_set_list
    # return url_set_list[:6], url_pdf_set_list
    # return url_set_list, url_pdf_set_list

def concat_url(threads):
    url_result = []
    for t in threads:
        tmp = t.join()
        if tmp:
            url_result.append(tmp)
    return url_result

async def get_url_text_list(keywords, search_url_num):
    start_time = record_time.time()

    async for chunk in get_search_url(keywords, search_url_num):
        if type(chunk) == str:
            yield chunk
        else:
            url_set_list, url_pdf_set_list = chunk
    # url_set_list, url_pdf_set_list = yield from get_search_url(keywords, search_url_num)

    yield "ğŸŒ message_search_stage_3"
    threads = []
    for url in url_set_list:
        # url_search_thread = ThreadWithReturnValue(target=jina_ai_Web_crawler, args=(url,True,))
        url_search_thread = ThreadWithReturnValue(target=Web_crawler, args=(url,True,))
        url_search_thread.start()
        threads.append(url_search_thread)

    url_text_list = concat_url(threads)

    yield "ğŸŒ message_search_stage_4"
    end_time = record_time.time()
    run_time = end_time - start_time
    print("urls", url_set_list)
    print(f"æœç´¢ç”¨æ—¶ï¼š{run_time}ç§’")

    yield url_text_list
    # return url_text_list

# Plugins æœç´¢å…¥å£
async def get_search_results(prompt: str, keywords):
    print("keywords", keywords)
    keywords = [item.replace("ä¸‰è¡Œå…³é”®è¯æ˜¯ï¼š", "") for item in keywords if "\\x" not in item if item != ""]
    keywords = [prompt] + keywords
    keywords = keywords[:3]
    print("select keywords", keywords)

    if len(keywords) == 3:
        search_url_num = 4
    if len(keywords) == 2:
        search_url_num = 6
    if len(keywords) == 1:
        search_url_num = 12

    url_text_list = []
    async for chunk in get_url_text_list(keywords, search_url_num):
        if type(chunk) == str:
            yield chunk
        else:
            url_text_list = chunk
        # url_text_list = yield chunk
    # url_text_list = yield from get_url_text_list(keywords, search_url_num)
    # useful_source_text = "\n\n".join(url_text_list)
    yield url_text_list
    # return useful_source_text

if __name__ == "__main__":
    os.system("clear")
    # from ModelMerge.models import chatgpt
    # print(get_search_results("ä»Šå¤©çš„å¾®åšçƒ­æœæœ‰å“ªäº›ï¼Ÿ", chatgpt.chatgpt_api_url.v1_url))

    # # æœç´¢

    # for i in search_web_and_summary("ä»Šå¤©çš„å¾®åšçƒ­æœæœ‰å“ªäº›ï¼Ÿ"):
    # for i in search_web_and_summary("ç»™å‡ºæ¸…åé“Šä¸­æ¯’æ¡ˆæ—¶é—´çº¿ï¼Œå¹¶ä½œå‡ºä½ çš„è¯„è®ºã€‚"):
    # for i in search_web_and_summary("çº¢è­¦hbk08æ˜¯è°"):
    # for i in search_web_and_summary("å›½åŠ¡é™¢ 2024 æ”¾å‡å®‰æ’"):
    # for i in search_web_and_summary("ä¸­å›½æœ€æ–°å…¬å¸ƒçš„æ¸¸æˆæ”¿ç­–ï¼Œå¯¹æ¸¸æˆè¡Œä¸šå’Œå…¶ä»–ç›¸å…³è¡Œä¸šæœ‰ä»€ä¹ˆæ ·çš„å½±å“ï¼Ÿ"):
    # for i in search_web_and_summary("ä»Šå¤©ä¸Šæµ·çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"):
    # for i in search_web_and_summary("é˜¿é‡Œäº‘24æ ¸96Gçš„äº‘ä¸»æœºä»·æ ¼æ˜¯å¤šå°‘"):
    # for i in search_web_and_summary("è¯è¯´è‘¬é€çš„èŠ™è‰è²åŠ¨æ¼«æ˜¯åŠå¹´ç•ªè¿˜æ˜¯å­£ç•ªï¼Ÿå®Œç»“æ²¡ï¼Ÿ"):
    # for i in search_web_and_summary("å‘¨æµ·åªšäº‹ä»¶è¿›å±•"):
    # for i in search_web_and_summary("macos 13.6 æœ‰ä»€ä¹ˆæ–°åŠŸèƒ½"):
    # for i in search_web_and_summary("ç”¨pythonå†™ä¸ªç½‘ç»œçˆ¬è™«ç»™æˆ‘"):
    # for i in search_web_and_summary("æ¶ˆå¤±çš„å¥¹ä¸»è¦è®²äº†ä»€ä¹ˆï¼Ÿ"):
    # for i in search_web_and_summary("å¥¥å·´é©¬çš„å…¨åæ˜¯ä»€ä¹ˆï¼Ÿ"):
    # for i in search_web_and_summary("åä¸ºmate60æ€ä¹ˆæ ·ï¼Ÿ"):
    # for i in search_web_and_summary("æ…ˆç¦§å…»çš„çŒ«å«ä»€ä¹ˆåå­—?"):
    # for i in search_web_and_summary("æ°‘è¿›å…šå½“åˆä¸ºä»€ä¹ˆæ”¯æŒæŸ¯æ–‡å“²é€‰å°åŒ—å¸‚é•¿ï¼Ÿ"):
    # for i in search_web_and_summary("Has the United States won the china US trade warï¼Ÿ"):
    # for i in search_web_and_summary("What does 'n+2' mean in Huawei's 'Mate 60 Pro' chipset? Please conduct in-depth analysis."):
    # for i in search_web_and_summary("AUTOMATIC1111 æ˜¯ä»€ä¹ˆï¼Ÿ"):
    # for i in search_web_and_summary("python telegram bot æ€ä¹ˆæ¥æ”¶pdfæ–‡ä»¶"):
    # for i in search_web_and_summary("ä¸­å›½åˆ©ç”¨å¤–èµ„æŒ‡æ ‡ä¸‹é™äº† 87% ï¼ŸçœŸçš„å‡çš„ã€‚"):
    # for i in search_web_and_summary("How much does the 'zeabur' software service cost per month? Is it free to use? Any limitations?"):
    # for i in search_web_and_summary("è‹±å›½è„±æ¬§æ²¡æœ‰å¥½å¤„ï¼Œä¸ºä»€ä¹ˆè‹±å›½äººè¿˜æ˜¯è¦è„±æ¬§ï¼Ÿ"):
    # for i in search_web_and_summary("2022å¹´ä¿„ä¹Œæˆ˜äº‰ä¸ºä»€ä¹ˆå‘ç”Ÿï¼Ÿ"):
    # for i in search_web_and_summary("å¡ç½—å°”ä¸æ˜ŸæœŸäºŒè®²çš„å•¥ï¼Ÿ"):
    # for i in search_web_and_summary("é‡‘ç –å›½å®¶ä¼šè®®æœ‰å“ªäº›å†³å®šï¼Ÿ"):
    # for i in search_web_and_summary("iphone15æœ‰å“ªäº›æ–°åŠŸèƒ½ï¼Ÿ"):
    # for i in search_web_and_summary("pythonå‡½æ•°å¼€å¤´ï¼šdef time(text: str) -> str:æ¯ä¸ªéƒ¨åˆ†æœ‰ä»€ä¹ˆç”¨ï¼Ÿ"):
        # print(i, end="")

    # é—®ç­”
    # result = asyncio.run(docQA("/Users/yanyuming/Downloads/GitHub/wiki/docs", "ubuntu ç‰ˆæœ¬å·æ€ä¹ˆçœ‹ï¼Ÿ"))
    # result = asyncio.run(docQA("https://yym68686.top", "è¯´ä¸€ä¸‹HSTL pipeline"))
    # result = asyncio.run(docQA("https://wiki.yym68686.top", "PyTorch to MindSporeç¿»è¯‘æ€è·¯æ˜¯ä»€ä¹ˆï¼Ÿ"))
    # print(result['answer'])
    # result = asyncio.run(pdfQA("https://api.telegram.org/file/bot5569497961:AAHobhUuydAwD8SPkXZiVFybvZJOmGrST_w/documents/file_1.pdf", "HSTLçš„pipelineè¯¦ç»†è®²ä¸€ä¸‹"))
    # print(result)
    # source_url = set([i.metadata['source'] for i in result["source_documents"]])
    # source_url = "\n".join(source_url)
    # message = (
    #     f"{result['result']}\n\n"
    #     f"å‚è€ƒé“¾æ¥ï¼š\n"
    #     f"{source_url}"
    # )
    # print(message)